@ARTICLE {graves-1995, 
	AUTHOR = {Graves, M. and Bergeman, E.R. and Lawrence, C.B.}, 
	JOURNAL = {Engineering in Medicine and Biology Magazine, IEEE}, 
	TITLE = {Graph database systems}, 
	YEAR = {1995},
	volume = {14}, 
	number = {6}, 
	PAGES = {737--745}, 
	ISSN = {0739--5175}}

@Article{Xu2017,
     Title = {Car Detection from Low-Altitude UAV Imagery with the Faster R-CNN},
     Author = {Xu, Yongzheng;Yu, Guizhen;Wang, Yunpeng;Wu, Xinkai;Ma, Yalong;},
     Journal = {Journal of Advanced Transportation},
     year = {2017},
     volume = {2017},
     doi = {10.1155/2017/2823617},
     url = {https://doi.org/10.1155/2017/2823617},
}

@INPROCEEDINGS{937593, 
author={Tao Zhao and R. Nevatia}, 
booktitle={Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001}, 
title={Car detection in low resolution aerial image}, 
year={2001}, 
volume={1}, 
number={}, 
pages={710-717 vol.1}, 
keywords={Bayes methods;image recognition;object recognition;3D object recognition problem;Bayesian network;car detection;human detection;low resolution aerial image;passenger cars detection;psychological tests;windshield;Image edge detection;Image resolution;Intelligent robots;Intelligent systems;Object detection;Object recognition;Psychology;Testing;Vehicle detection;Vehicles}, 
doi={10.1109/ICCV.2001.937593}, 
ISSN={}, 
month={},}

@INPROCEEDINGS{7780459, 
author={K. He and X. Zhang and S. Ren and J. Sun}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Deep Residual Learning for Image Recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={770-778}, 
keywords={image classification;learning (artificial intelligence);neural nets;object detection;CIFAR-10;COCO object detection dataset;COCO segmentation;ILSVRC & COCO 2015 competitions;ILSVRC 2015 classification task;ImageNet dataset;ImageNet localization;ImageNet test set;VGG nets;deep residual learning;deep residual nets;deeper neural network training;image recognition;residual function learning;residual nets;visual recognition tasks;Complexity theory;Degradation;Image recognition;Image segmentation;Neural networks;Training;Visualization}, 
doi={10.1109/CVPR.2016.90}, 
ISSN={}, 
month={June},}

@inproceedings{Hsieh_2017_ICCV, 
Author = {Meng-Ru Hsieh and Yen-Liang Lin and Winston H. Hsu}, 
Booktitle = {The IEEE International Conference on Computer Vision (ICCV)}, 
Title = {Drone-based Object Counting by Spatially Regularized Regional Proposal Networks}, 
Year = {2017}, 
organization={IEEE} 
}

@article{DBLP:journals/corr/abs-1710-09829,
  author    = {Sara Sabour and
               Nicholas Frosst and
               Geoffrey E. Hinton},
  title     = {Dynamic Routing Between Capsules},
  journal   = {CoRR},
  volume    = {abs/1710.09829},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.09829},
  archivePrefix = {arXiv},
  eprint    = {1710.09829},
  timestamp = {Thu, 02 Nov 2017 14:25:36 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-09829},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1710-09829,
  author    = {Sara Sabour and
               Nicholas Frosst and
               Geoffrey E. Hinton},
  title     = {Dynamic Routing Between Capsules},
  journal   = {CoRR},
  volume    = {abs/1710.09829},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.09829},
  archivePrefix = {arXiv},
  eprint    = {1710.09829},
  timestamp = {Thu, 02 Nov 2017 14:25:36 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-09829},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/JegouDVRB16,
  author    = {Simon J{\'{e}}gou and
               Michal Drozdzal and
               David V{\'{a}}zquez and
               Adriana Romero and
               Yoshua Bengio},
  title     = {The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for
               Semantic Segmentation},
  journal   = {CoRR},
  volume    = {abs/1611.09326},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09326},
  archivePrefix = {arXiv},
  eprint    = {1611.09326},
  timestamp = {Wed, 07 Jun 2017 14:40:48 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/JegouDVRB16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Li2018,
author="Li, Yu
and Qian, Meiyu
and Liu, Pengfeng
and Cai, Qian
and Li, Xiaoying
and Guo, Junwen
and Yan, Huan
and Yu, Fengyuan
and Yuan, Kun
and Yu, Juan
and Qin, Luman
and Liu, Hongxin
and Wu, Wan
and Xiao, Peiyun
and Zhou, Ziwei",
title="The recognition of rice images by UAV based on capsule network",
journal="Cluster Computing",
year="2018",
month="Mar",
day="16",
abstract="It is important to recognize the rice image captured by unmanned aerial vehicle (UAV) for monitoring the growth of rice and preventing the diseases and pests. Aiming at the image recognition, we use rice images captured by UAV as our data source, the structure of capsule network (CapsNet) is built to recognize rice images in this paper. The images are preprocessed through histogram equalization method into grayscale images and through superpixel algorithm into the superpixel segmentation results. The both results are output into the CapsNet. The function of CapsNet is to perform the reverse analysis of rice images. The CapsNet consists of five layers: an input layer, a convolution layer, a primary capsules layer, a digital capsules layer and an output layer. The CapsNet trains classification and predicts the output vector based on routing-by-agreement protocol. Therefore, the features of rice image by UAV can be precisely and efficiently extracted. The method is more convenient than the traditional artificial recognition. It provides the scientific support and reference for decision-making process of precision agriculture.",
issn="1573-7543",
doi="10.1007/s10586-018-2482-7",
url="https://doi.org/10.1007/s10586-018-2482-7"
}


@Article{jimaging3020021,
AUTHOR = {Radovic, Matija and Adarkwa, Offei and Wang, Qiaosong},
TITLE = {Object Recognition in Aerial Images Using Convolutional Neural Networks},
JOURNAL = {Journal of Imaging},
VOLUME = {3},
YEAR = {2017},
NUMBER = {2},
URL = {http://www.mdpi.com/2313-433X/3/2/21},
ISSN = {2313-433X},
ABSTRACT = {There are numerous applications of unmanned aerial vehicles (UAVs) in the management of civil infrastructure assets. A few examples include routine bridge inspections, disaster management, power line surveillance and traffic surveying. As UAV applications become widespread, increased levels of autonomy and independent decision-making are necessary to improve the safety, efficiency, and accuracy of the devices. This paper details the procedure and parameters used for the training of convolutional neural networks (CNNs) on a set of aerial images for efficient and automated object recognition. Potential application areas in the transportation field are also highlighted. The accuracy and reliability of CNNs depend on the network’s training and the selection of operational parameters. This paper details the CNN training procedure and parameter selection. The object recognition results show that by selecting a proper set of parameters, a CNN can detect and classify objects with a high level of accuracy (97.5%) and computational efficiency. Furthermore, using a convolutional neural network implemented in the “YOLO” (“You Only Look Once”) platform, objects can be tracked, detected (“seen”), and classified (“comprehended”) from video feeds supplied by UAVs in real-time.},
DOI = {10.3390/jimaging3020021}
}

@Article{rs9040312,
AUTHOR = {Ammour, Nassim and Alhichri, Haikel and Bazi, Yakoub and Benjdira, Bilel and Alajlan, Naif and Zuair, Mansour},
TITLE = {Deep Learning Approach for Car Detection in UAV Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {9},
YEAR = {2017},
NUMBER = {4},
URL = {http://www.mdpi.com/2072-4292/9/4/312},
ISSN = {2072-4292},
ABSTRACT = {This paper presents an automatic solution to the problem of detecting and counting cars in unmanned aerial vehicle (UAV) images. This is a challenging task given the very high spatial resolution of UAV images (on the order of a few centimetres) and the extremely high level of detail, which require suitable automatic analysis methods. Our proposed method begins by segmenting the input image into small homogeneous regions, which can be used as candidate locations for car detection. Next, a window is extracted around each region, and deep learning is used to mine highly descriptive features from these windows. We use a deep convolutional neural network (CNN) system that is already pre-trained on huge auxiliary data as a feature extraction tool, combined with a linear support vector machine (SVM) classifier to classify regions into “car” and “no-car” classes. The final step is devoted to a fine-tuning procedure which performs morphological dilation to smooth the detected regions and fill any holes. In addition, small isolated regions are analysed further using a few sliding rectangular windows to locate cars more accurately and remove false positives. To evaluate our method, experiments were conducted on a challenging set of real UAV images acquired over an urban area. The experimental results have proven that the proposed method outperforms the state-of-the-art methods, both in terms of accuracy and computational time.},
DOI = {10.3390/rs9040312}
}


